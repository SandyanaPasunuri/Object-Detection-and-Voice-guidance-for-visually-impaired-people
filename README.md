This project focuses on developing an advanced object detection system tailored for aiding visually impaired individuals in indoor environments. The system utilizes the YOLO (You Only Look Once) object detection model to accurately identify and track objects such as indoor signage, stairs, and pedestrians in real-time. By integrating the GTTS (Google Text-to-Speech) module, the application converts detected object information into voice alerts, thereby providing immediate auditory feedback to the user. For training and testing, the model was trained on a dataset consisting of 5,000 images and tested on 1,000 images, ensuring robust and reliable performance. This real-time guidance system enhances the mobility and situational awareness of visually impaired users, making indoor navigation safer and more efficient. The experimental results demonstrate the system's rapid detection capabilities and effective voice guidance, showcasing its potential as a valuable tool for improving the quality of life for visually impaired individuals.
